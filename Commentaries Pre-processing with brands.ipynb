{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commentaries file pre-processing with brands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing commentaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper_save_load import load_excel_spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Forecast and Actual sheets\n",
    "#dfc = load_excel_spreadsheet('./datasets/Commentaries dataset v2.xlsm', 'All')\n",
    "\n",
    "\n",
    "#Import dataframes to pickle file (saved previously)\n",
    "\n",
    "from helper_save_load import load_from_pickle\n",
    "dfc = load_from_pickle(\"commentaries_with_brands.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting month names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_name=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "def format_month(m):\n",
    "    ym = m.split('/')\n",
    "    return(month_name[int(ym[1])-1]+'_'+ym[0])\n",
    "dfc['Month_f'] = dfc['Month'].apply(format_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning commentaries\n",
    "- Removing punctuation, indesirable words or sentences and copy new comment to a new column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a sample of commentaries\n",
    "df_sample = dfc     #.loc[0:100,:]\n",
    "NotNull = df_sample['Commentaries'].notna()\n",
    "df_comments = df_sample[NotNull].loc[:,'Commentaries']\n",
    "df_comments.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining codes\n",
    "SOS = '[SOS]'\n",
    "EOS = '[EOS]'\n",
    "NOC = '[NOC]'\n",
    "MILLIONS_NUM = '[#M]'\n",
    "THOUSAND_NUM = '[#K]'\n",
    "NUMBER = '[#]'\n",
    "PERCENT = '[%]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding space after +/-, deleting (,),: , deleting 'please see comments above', \n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Replace patterns of numbers by tokens\n",
    "resubs = [(r\"(\\$)?[0-9](\\.[0-9]+)?M\\b\", MILLIONS_NUM),\n",
    "         (r\"(\\$)?[0-9]+[Kk]\\b\", THOUSAND_NUM),\n",
    "         (r\"[0-9]+(\\.[0-9]+)?\\%\", PERCENT),\n",
    "         (r\"\\$[0-9]+(\\.[0-9]+)?\\b\", NUMBER)]\n",
    "\n",
    "raplacements = [('+ [#','+[#'), ('- [#','-[#'),\n",
    "                ('+','+'),\n",
    "                ('-','-'),\n",
    "                ('(',' '),\n",
    "                (')',' '),\n",
    "                (':',' '),\n",
    "                (',',' '),\n",
    "                ('.',' '),\n",
    "                (';',''),\n",
    "                ('\\n',' '),\n",
    "                ('0ml ','0 ml '), ('3ml ','3 ml '), ('4ml ','4 ml '), ('5ml ','5 ml '),\n",
    "                ('0g ','0 g '), ('3g ','3 g '), ('5g ','5 g '), ('6g ','6 g '),\n",
    "                ('liption','lipton'),\n",
    "                ('please see comments above', ''),\n",
    "                ('please see comments below', ''),\n",
    "                ('Please see comments above',''),\n",
    "                ('Please see comments below',''),\n",
    "               ]\n",
    "\n",
    "for i, comment in zip(df_comments.index,df_comments):\n",
    "    if (comment==NOC):\n",
    "        df_sample.loc[i,'Comment_w'] = NOC\n",
    "    else:\n",
    "        for r in resubs:\n",
    "            comment = re.sub(r[0],r[1], comment)\n",
    "        comment = comment.lower()\n",
    "        for r in raplacements:\n",
    "            comment = comment.replace(r[0],r[1])\n",
    "        result = re.split(' |/|:', comment)\n",
    "\n",
    "        result2 = [ps.stem(m) for m in result if m!='']\n",
    "        df_sample.loc[i,'Comment_w'] = SOS + ' ' + ' '.join(result2) + ' ' + EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_com = df_sample[NotNull].loc[:,'Comment_w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_corpus = ' '.join(df_all_com.ravel()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[NotNull].loc[0:190,['Commentaries', 'Comment_w']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1499f77ba10>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab : 678\n",
      "Size of dictionary : 678\n"
     ]
    }
   ],
   "source": [
    "#Create vocabulary and dictionary\n",
    "vocab = set(com_corpus)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "print('Size of vocab :', len(vocab))\n",
    "print('Size of dictionary :', len(word_to_ix))\n",
    "\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "#trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "#            for i in range(len(test_sentence) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in sorted(vocab):\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dictionnary and commentaries dataframe to pickle file\n",
    "from helper_save_load import save_to_pickle\n",
    "save_to_pickle(\"commentaries.pickle\", (dfc, vocab, word_to_ix, ix_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word embedding using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOC]\n",
      "\n",
      "Examples of trigrams for training:\n",
      "(['[SOS]', 'driven'], 'by')\n",
      "(['driven', 'by'], 'jc')\n",
      "(['by', 'jc'], 'sobey')\n",
      "(['jc', 'sobey'], 'metro')\n"
     ]
    }
   ],
   "source": [
    "#Create tri-grams from all comments [word1,word2,target]\n",
    "trigrams = []\n",
    "for comment in df_all_com:\n",
    "    comment_list = comment.split()\n",
    "    tri = [([comment_list[i], comment_list[i + 1]], comment_list[i + 2])\n",
    "            for i in range(len(comment_list) - 2)]\n",
    "    for tr in tri:\n",
    "        trigrams.append(tr)\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(df_all_com[df_all_com.index[0]])\n",
    "print('\\nExamples of trigrams for training:')\n",
    "for tri in trigrams[:4]:\n",
    "    print(tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) #Embedding matrix: each line is the embedding of one word\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128) #Parameter matrix embedding and hidden layer\n",
    "        self.linear2 = nn.Linear(128, vocab_size)  #Parameter matrix between hidden layer and output\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))  #get embedding from Embedding matrix\n",
    "        out = F.relu(self.linear1(embeds))  #\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()   #Negative log likelihood loss (multiclass output with softmax)\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  #stochastic gradient descent   #before 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : Total loss=6019.817\n",
      "epoch 1 : Total loss=5961.326\n",
      "epoch 2 : Total loss=5914.249\n",
      "epoch 3 : Total loss=5845.504\n",
      "epoch 4 : Total loss=5791.026\n",
      "epoch 5 : Total loss=5733.826\n",
      "epoch 6 : Total loss=5701.335\n",
      "epoch 7 : Total loss=5647.558\n",
      "epoch 8 : Total loss=5587.535\n",
      "epoch 9 : Total loss=5558.567\n",
      "epoch 10 : Total loss=5533.625\n",
      "epoch 11 : Total loss=5473.613\n",
      "epoch 12 : Total loss=5450.304\n",
      "epoch 13 : Total loss=5436.706\n",
      "epoch 14 : Total loss=5391.777\n",
      "epoch 15 : Total loss=5351.454\n",
      "epoch 16 : Total loss=5325.113\n",
      "epoch 17 : Total loss=5289.418\n",
      "epoch 18 : Total loss=5248.416\n",
      "epoch 19 : Total loss=5238.990\n"
     ]
    }
   ],
   "source": [
    "#Training, you can re-run this function as much time as needed to train more\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "    print('epoch %d : Total loss=%.3f' % (epoch, total_loss))\n",
    "#print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['[#]', 'fp']\n",
      "Target: ad\n",
      "Prediction: ad\n"
     ]
    }
   ],
   "source": [
    "context, target = trigrams[133]\n",
    "print('Context:', context)\n",
    "print('Target:', target)\n",
    "\n",
    "#Predict next word using model\n",
    "context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "log_probs = model(context_idxs)\n",
    "\n",
    "#Get the word that has max probability and display it\n",
    "result = log_probs.data.numpy().tolist()[0]   #Convert tensor to list\n",
    "indexmax = result.index(np.max(result)) #get index of the greatest probable word\n",
    "print('Prediction:', ix_to_word[indexmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful function to predict next word, calculate distance between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(context):\n",
    "    #Predict next word using model\n",
    "    context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "    log_probs = model(context_idxs)\n",
    "\n",
    "    #Get the word that has max probability and display it\n",
    "    result = log_probs.data.numpy().tolist()[0]   #Convert tensor to list\n",
    "    indexmax = result.index(np.max(result)) #get index of the greatest probable word\n",
    "    return (ix_to_word[indexmax])\n",
    "\n",
    "def embedding_word(word):\n",
    "    word_idx = torch.tensor([word_to_ix[word]], dtype=torch.long)\n",
    "    return(model.embeddings(word_idx).data.numpy().tolist()[0])\n",
    "\n",
    "def distance_words_pytorch(word1,word2):    #between -1 and 1\n",
    "    from scipy import spatial\n",
    "    return(1-spatial.distance.cosine(embedding_word(word1), embedding_word(word2)))\n",
    "\n",
    "def predict_next_multi(context, topn):\n",
    "    #Predict next word using model\n",
    "    context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "    log_probs = model(context_idxs)\n",
    "\n",
    "    #Get the word that has max probability and display it\n",
    "    result = log_probs.data.numpy().tolist()[0]   #Convert tensor to list\n",
    "    result_s = sorted(result, reverse=True)\n",
    "    \n",
    "    mydict = [(ix_to_word[result.index(p)], np.exp(p)) for p in result_s[0:topn]]\n",
    "    return (mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pallet', 0.9846269512220999),\n",
       " ('order', 0.006120203477839846),\n",
       " ('baselin', 0.001555282882754302),\n",
       " ('+[#m]', 0.0013861285217311454),\n",
       " ('wm', 0.0013247650488891853),\n",
       " ('due', 0.0009583547102707161),\n",
       " ('of', 0.0006802531696448085),\n",
       " ('[#m]', 0.0005132653876698086),\n",
       " ('east', 0.0004776263691683376),\n",
       " ('bundl', 0.00031231600188834596)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_multi(['by', 'half'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strong', 0.05640051200655534),\n",
       " ('sdm', 0.05501668215557816),\n",
       " ('dove', 0.053456151132948986),\n",
       " ('pipelin', 0.040397351595921914),\n",
       " ('overlay', 0.03554975434339591),\n",
       " ('lcl', 0.034128397656997385),\n",
       " ('500ct', 0.03342795889283765),\n",
       " ('phase', 0.032160861630735615),\n",
       " ('promot', 0.03123870666333585),\n",
       " ('ooh', 0.030193855815927418)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_multi(['driven', 'by'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[EOS]', 0.2323307453492714),\n",
       " ('+[#m]', 0.16541634117325493),\n",
       " ('easter', 0.08454624635488212),\n",
       " ('60ct', 0.07000552454248384),\n",
       " ('[#k]', 0.055112651024521626),\n",
       " ('+[#k]', 0.03319487624214638),\n",
       " ('-[#m]', 0.019055897469822402),\n",
       " ('due', 0.013317986948961483),\n",
       " ('east', 0.010084910588059638),\n",
       " ('and', 0.009707428581251957)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_multi(['by', 'lcl'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next(['[SOS]','driven'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sh'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next(['expert','segment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deliveri +[#m] sdm oct order phase in sep 6pack bonu po growth on 355 ml\n"
     ]
    }
   ],
   "source": [
    "#Basic commentary generation from distance between words\n",
    "word1 = 'over'\n",
    "word2 = 'deliveri'\n",
    "word=''\n",
    "sentence = [word2]\n",
    "i=0\n",
    "while (word!= EOS) & (i<20):\n",
    "    word = predict_next([word1, word2])\n",
    "    sentence.append(word)\n",
    "    word1 = word2\n",
    "    word2 = word\n",
    "    i += 1\n",
    "    \n",
    "print(' '.join(sentence[0:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closet_words_pytorch(target, topn):   #Does not work well based on the embedding distances\n",
    "    import operator\n",
    "    mydict = [(m, distance_words_pytorch(target,m)) for m in list(vocab)]\n",
    "    sorted_tuples = sorted(mydict, key=operator.itemgetter(1), reverse=True)\n",
    "    return(sorted_tuples[1:topn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faster', 0.22281482467763603),\n",
       " ('uniprix', 0.18852167930799202),\n",
       " ('deo', 0.17616654634481688),\n",
       " ('pog', 0.16812810407354695),\n",
       " ('materi', 0.1658039219143258),\n",
       " ('off', 0.1585459230168571),\n",
       " ('performac', 0.15685728585517456),\n",
       " ('miss', 0.15067718002520358),\n",
       " ('24', 0.14896671444657628)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closet_words_pytorch('driven',10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22757552606022324"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_words_pytorch('sobey','lcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3756,  0.0396,  0.1490,  0.1667, -0.8133,  0.0488,  0.7215, -0.6928,\n",
       "         -1.5910, -1.1907, -0.0978, -0.0095, -0.9462, -1.2306, -0.1153, -0.1944,\n",
       "         -0.4340,  0.1308, -0.8175,  0.4913]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'doh'\n",
    "word_to_ix[word]\n",
    "word_idx = torch.tensor([word_to_ix[word]], dtype=torch.long)\n",
    "model.embeddings(word_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(model.state_dict(), './embedding_model_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NGramLanguageModeler(\n",
       "  (embeddings): Embedding(932, 20)\n",
       "  (linear1): Linear(in_features=40, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=932, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "#The model class should be defined\n",
    "model.load_state_dict(torch.load('./embedding_model_params'))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
